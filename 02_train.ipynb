{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 02_train\n",
        "\n",
        "Train U-Net for glacier segmentation using preprocessed tensors.\n",
        "\n",
        "- Loads preprocessed `.npz` or `.pt` created by `01_preprocess.ipynb`\n",
        "- Supports 5 or 6 input channels (with optional NDSI)\n",
        "- Dice + BCE loss\n",
        "- Logs Dice, IoU, MCC per epoch\n",
        "- Saves best checkpoint by validation MCC\n",
        "\n",
        "```python\n",
        "# Colab setup (uncomment if running on Colab)\n",
        "# !pip -q install torch torchvision numpy pandas scikit-learn opencv-python pillow tqdm matplotlib rasterio\n",
        "# from pathlib import Path\n",
        "# for p in [\"data/images\",\"data/masks\",\"data/preprocessed\",\"checkpoints\",\"results\"]:\n",
        "#     Path(p).mkdir(parents=True, exist_ok=True)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    data_dir: Path = Path(\"data/preprocessed\")\n",
        "    file_ext: str = \".npz\"  # or \".pt\"\n",
        "    region_filter: str = \"RegionA\"  # files containing this substring\n",
        "    input_channels: int = 6  # 5 or 6 (if NDSI used)\n",
        "    num_classes: int = 2\n",
        "    epochs: int = 50\n",
        "    batch_size: int = 4\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    val_fraction: float = 0.2\n",
        "    num_workers: int = 2\n",
        "    save_dir: Path = Path(\"checkpoints\")\n",
        "\n",
        "\n",
        "cfg = TrainConfig()\n",
        "cfg.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "set_seed(42)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GlacierTensorDataset(Dataset):\n",
        "    def __init__(self, directory: Path, file_ext: str, region_filter: str | None = None):\n",
        "        self.paths = sorted([p for p in directory.glob(f\"*{file_ext}\") if (region_filter is None or region_filter in p.name)])\n",
        "        if len(self.paths) == 0:\n",
        "            raise FileNotFoundError(f\"No files found in {directory} with ext {file_ext} and filter {region_filter}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        path = self.paths[idx]\n",
        "        if path.suffix == \".npz\":\n",
        "            data = np.load(path)\n",
        "            image = data[\"image\"].astype(np.float32)\n",
        "            mask = data[\"mask\"].astype(np.uint8)\n",
        "        elif path.suffix == \".pt\":\n",
        "            data = torch.load(path)\n",
        "            image = data[\"image\"].numpy().astype(np.float32)\n",
        "            mask = data[\"mask\"].numpy().astype(np.uint8)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported extension: \" + path.suffix)\n",
        "        # Convert to torch tensors\n",
        "        x = torch.from_numpy(image)\n",
        "        y = torch.from_numpy(mask).long()\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train/val (Region A subset)\n",
        "full_ds = GlacierTensorDataset(cfg.data_dir, cfg.file_ext, cfg.region_filter)\n",
        "if len(full_ds) == 0:\n",
        "    print(f\"Warning: No files found in {cfg.data_dir} with extension {cfg.file_ext} and filter {cfg.region_filter}\")\n",
        "else:\n",
        "    sample_x, _ = full_ds[0]\n",
        "    assert sample_x.shape[0] == cfg.input_channels, \\\n",
        "        f\"Mismatch: data has {sample_x.shape[0]} channels but cfg.input_channels={cfg.input_channels}. Update config.\"\n",
        "\n",
        "val_len = int(len(full_ds) * cfg.val_fraction)\n",
        "train_len = len(full_ds) - val_len\n",
        "train_ds, val_ds = random_split(full_ds, [train_len, val_len])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "\n",
        "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal U-Net adapted to variable input channels\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
        "        self.dropout = nn.Dropout2d(p=dropout)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dropout(self.conv1(x)))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
        "        self.conv = ConvBlock(in_ch, out_ch, dropout)\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip, x], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels: int, num_classes: int = 2, base_ch: int = 32, depth: int = 4, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        ch_in = in_channels\n",
        "        ch_out = base_ch\n",
        "        for _ in range(depth):\n",
        "            self.downs.append(ConvBlock(ch_in, ch_out, dropout))\n",
        "            ch_in, ch_out = ch_out, ch_out * 2\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.mid = ConvBlock(ch_in, ch_out, dropout)\n",
        "        ch_in, ch_out = ch_out, ch_out // 2\n",
        "        for _ in range(depth):\n",
        "            self.ups.append(UpBlock(ch_in, ch_out, dropout))\n",
        "            ch_in, ch_out = ch_out, ch_out // 2\n",
        "        self.seg = nn.Conv2d(ch_in * 2, num_classes, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        for block in self.downs:\n",
        "            x = block(x)\n",
        "            skips.append(x)\n",
        "            x = self.pool(x)\n",
        "        x = self.mid(x)\n",
        "        for block in self.ups:\n",
        "            x = block(x, skips.pop())\n",
        "        x = self.seg(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Losses and metrics\n",
        "\n",
        "def dice_coefficient(prob: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    # prob: (N,1,H,W) or (N,2,H,W) logits later sigmoid/softmax\n",
        "    if prob.shape[1] == 2:\n",
        "        prob_bin = torch.softmax(prob, dim=1)[:, 1]\n",
        "    else:\n",
        "        prob_bin = torch.sigmoid(prob[:, 0])\n",
        "    target = target.float()\n",
        "    intersection = (prob_bin * target).sum(dim=(1,2))\n",
        "    union = prob_bin.sum(dim=(1,2)) + target.sum(dim=(1,2))\n",
        "    dice = (2 * intersection + eps) / (union + eps)\n",
        "    return dice.mean()\n",
        "\n",
        "\n",
        "def iou_score(prob: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    if prob.shape[1] == 2:\n",
        "        prob_bin = (torch.softmax(prob, dim=1)[:, 1] > 0.5).float()\n",
        "    else:\n",
        "        prob_bin = (torch.sigmoid(prob[:, 0]) > 0.5).float()\n",
        "    target = target.float()\n",
        "    intersection = (prob_bin * target).sum(dim=(1,2))\n",
        "    union = (prob_bin + target).clamp(0,1).sum(dim=(1,2))\n",
        "    return ((intersection + eps) / (union + eps)).mean()\n",
        "\n",
        "\n",
        "def matthews_corrcoef(prob: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    if prob.shape[1] == 2:\n",
        "        pred = (torch.softmax(prob, dim=1)[:, 1] > 0.5).float()\n",
        "    else:\n",
        "        pred = (torch.sigmoid(prob[:, 0]) > 0.5).float()\n",
        "    target = target.float()\n",
        "    tp = (pred * target).sum(dim=(1,2))\n",
        "    tn = ((1 - pred) * (1 - target)).sum(dim=(1,2))\n",
        "    fp = (pred * (1 - target)).sum(dim=(1,2))\n",
        "    fn = ((1 - pred) * target).sum(dim=(1,2))\n",
        "    numerator = (tp * tn - fp * fn)\n",
        "    denominator = torch.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + eps)\n",
        "    return (numerator / (denominator + eps)).mean()\n",
        "\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight_dice: float = 0.5, weight_bce: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.weight_dice = weight_dice\n",
        "        self.weight_bce = weight_bce\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        if logits.shape[1] == 2:\n",
        "            # use CE for 2-class logits\n",
        "            ce_loss = self.ce(logits, target)\n",
        "            # compute dice on foreground probs\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1:2]\n",
        "            target_f = target.float().unsqueeze(1)\n",
        "            intersection = (probs * target_f).sum(dim=(1,2,3))\n",
        "            union = probs.sum(dim=(1,2,3)) + target_f.sum(dim=(1,2,3))\n",
        "            dice = (2 * intersection + 1e-6) / (union + 1e-6)\n",
        "            dice_loss = 1 - dice.mean()\n",
        "            return self.weight_dice * dice_loss + self.weight_bce * ce_loss\n",
        "        else:\n",
        "            # single-channel logits\n",
        "            bce_loss = self.bce(logits[:, 0], target.float())\n",
        "            probs = torch.sigmoid(logits[:, 0:1])\n",
        "            target_f = target.float().unsqueeze(1)\n",
        "            intersection = (probs * target_f).sum(dim=(1,2,3))\n",
        "            union = probs.sum(dim=(1,2,3)) + target_f.sum(dim=(1,2,3))\n",
        "            dice = (2 * intersection + 1e-6) / (union + 1e-6)\n",
        "            dice_loss = 1 - dice.mean()\n",
        "            return self.weight_dice * dice_loss + self.weight_bce * bce_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and validation loops\n",
        "\n",
        "def run_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer | None, device: str, criterion: nn.Module) -> Dict[str, float]:\n",
        "    train = optimizer is not None\n",
        "    model.train(train)\n",
        "    total_loss = 0.0\n",
        "    total_dice = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_mcc = 0.0\n",
        "    count = 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            total_loss += float(loss.detach().cpu().item())\n",
        "            total_dice += float(dice_coefficient(logits.detach(), y).cpu().item())\n",
        "            total_iou += float(iou_score(logits.detach(), y).cpu().item())\n",
        "            total_mcc += float(matthews_corrcoef(logits.detach(), y).cpu().item())\n",
        "            count += 1\n",
        "    return {\n",
        "        \"loss\": total_loss / max(1, count),\n",
        "        \"dice\": total_dice / max(1, count),\n",
        "        \"iou\": total_iou / max(1, count),\n",
        "        \"mcc\": total_mcc / max(1, count),\n",
        "    }\n",
        "\n",
        "\n",
        "model = UNet(in_channels=cfg.input_channels, num_classes=2).to(cfg.device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "criterion = DiceBCELoss()\n",
        "\n",
        "best_mcc = -1.0\n",
        "best_path = cfg.save_dir / \"best_mcc.pt\"\n",
        "\n",
        "for epoch in range(1, cfg.epochs + 1):\n",
        "    train_metrics = run_epoch(model, train_loader, optimizer, cfg.device, criterion)\n",
        "    val_metrics = run_epoch(model, val_loader, None, cfg.device, criterion)\n",
        "    print(f\"Epoch {epoch:03d} | train loss {train_metrics['loss']:.4f} dice {train_metrics['dice']:.4f} iou {train_metrics['iou']:.4f} mcc {train_metrics['mcc']:.4f} | val loss {val_metrics['loss']:.4f} dice {val_metrics['dice']:.4f} iou {val_metrics['iou']:.4f} mcc {val_metrics['mcc']:.4f}\")\n",
        "    if val_metrics[\"mcc\"] > best_mcc:\n",
        "        best_mcc = val_metrics[\"mcc\"]\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"val_metrics\": val_metrics,\n",
        "            \"config\": vars(cfg)\n",
        "        }, best_path)\n",
        "        print(f\"Saved new best checkpoint (MCC={best_mcc:.4f}) -> {best_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
